{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3mkz27mwKe/jlt/ZW1FQV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VaishnaviDevi08/Brain-Tumor-Analysis-And-Prediction/blob/main/DL_ASSIGNMENT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jRMNmu79XU9"
      },
      "outputs": [],
      "source": [
        "#DL ASSIGNMENT1\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Activation functions\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "# Derivative of activation functions\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "# Loss functions\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    m = y_true.shape[0]\n",
        "    return -np.sum(y_true * np.log(y_pred + 1e-15)) / m\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    m = y_true.shape[0]\n",
        "    return np.sum((y_true - y_pred) ** 2) / m\n",
        "\n",
        "# Neural Network Class\n",
        "class FeedforwardNeuralNetwork:\n",
        "    def __init__(self, layer_sizes, activation='relu', weight_init='xavier'):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.activation = activation\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        self.weight_init = weight_init\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        for i in range(len(self.layer_sizes) - 1):\n",
        "            if self.weight_init == 'xavier':\n",
        "                limit = np.sqrt(6 / (self.layer_sizes[i] + self.layer_sizes[i + 1]))\n",
        "                self.weights.append(np.random.uniform(-limit, limit, (self.layer_sizes[i], self.layer_sizes[i + 1])))\n",
        "            else:\n",
        "                self.weights.append(np.random.randn(self.layer_sizes[i], self.layer_sizes[i + 1]) * 0.01)\n",
        "            self.biases.append(np.zeros((1, self.layer_sizes[i + 1])))\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.activations = [X]\n",
        "        self.z_values = []\n",
        "        for i in range(len(self.weights)):\n",
        "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
        "            self.z_values.append(z)\n",
        "            if i == len(self.weights) - 1:\n",
        "                a = softmax(z)  # Output layer\n",
        "            else:\n",
        "                a = relu(z) if self.activation == 'relu' else sigmoid(z)\n",
        "            self.activations.append(a)\n",
        "        return self.activations[-1]\n",
        "\n",
        "    def backward(self, X, y, learning_rate, optimizer='sgd', momentum=0.9, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        m = X.shape[0]\n",
        "        gradients = []\n",
        "        delta = self.activations[-1] - y  # Output layer error\n",
        "        for i in reversed(range(len(self.weights))):\n",
        "            dw = np.dot(self.activations[i].T, delta) / m\n",
        "            db = np.sum(delta, axis=0, keepdims=True) / m\n",
        "            gradients.append((dw, db))\n",
        "            if i > 0:\n",
        "                delta = np.dot(delta, self.weights[i].T) * (relu_derivative(self.activations[i]) if self.activation == 'relu' else sigmoid_derivative(self.activations[i]))\n",
        "\n",
        "        # Update weights and biases\n",
        "        for i in range(len(self.weights)):\n",
        "            if optimizer == 'sgd':\n",
        "                self.weights[i] -= learning_rate * gradients[-(i + 1)][0]\n",
        "                self.biases[i] -= learning_rate * gradients[-(i + 1)][1]\n",
        "            elif optimizer == 'momentum':\n",
        "                if not hasattr(self, 'velocity'):\n",
        "                    self.velocity = [np.zeros_like(w) for w in self.weights]\n",
        "                self.velocity[i] = momentum * self.velocity[i] - learning_rate * gradients[-(i + 1)][0]\n",
        "                self.weights[i] += self.velocity[i]\n",
        "                self.biases[i] -= learning_rate * gradients[-(i + 1)][1]\n",
        "            elif optimizer == 'adam':\n",
        "                if not hasattr(self, 'm'):\n",
        "                    self.m = [np.zeros_like(w) for w in self.weights]\n",
        "                    self.v = [np.zeros_like(w) for w in self.weights]\n",
        "                self.m[i] = beta1 * self.m[i] + (1 - beta1) * gradients[-(i + 1)][0]\n",
        "                self.v[i] = beta2 * self.v[i] + (1 - beta2) * (gradients[-(i + 1)][0] ** 2)\n",
        "                m_hat = self.m[i] / (1 - beta1 ** (i + 1))\n",
        "                v_hat = self.v[i] / (1 - beta2 ** (i + 1))\n",
        "                self.weights[i] -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
        "                self.biases[i] -= learning_rate * gradients[-(i + 1)][1]\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val, epochs=10, batch_size=32, learning_rate=0.001, optimizer='sgd', weight_decay=0):\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(0, X_train.shape[0], batch_size):\n",
        "                X_batch = X_train[i:i + batch_size]\n",
        "                y_batch = y_train[i:i + batch_size]\n",
        "                y_pred = self.forward(X_batch)\n",
        "                self.backward(X_batch, y_batch, learning_rate, optimizer)\n",
        "                if weight_decay > 0:\n",
        "                    for w in self.weights:\n",
        "                        w -= learning_rate * weight_decay * w\n",
        "\n",
        "            y_val_pred = self.forward(X_val)\n",
        "            val_loss = cross_entropy_loss(y_val, y_val_pred)\n",
        "            val_acc = accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1))\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        y_pred = self.forward(X_test)\n",
        "        test_acc = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n",
        "        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "        return y_pred\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and preprocess dataset (replace with Caltech-256 or MNIST)\n",
        "    from sklearn.datasets import load_digits\n",
        "\n",
        "    data = load_digits()\n",
        "    X = data.data\n",
        "    y = data.target.reshape(-1, 1)\n",
        "    y = OneHotEncoder(sparse_output=False).fit_transform(y)  # Fix: Use sparse_output instead of sparse\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "    # Define and train the model\n",
        "    model = FeedforwardNeuralNetwork(layer_sizes=[X.shape[1], 64, 32, y.shape[1]], activation='relu', weight_init='xavier')\n",
        "    model.train(X_train, y_train, X_val, y_val, epochs=10, batch_size=32, learning_rate=0.001, optimizer='adam', weight_decay=0.0005)\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_pred = model.evaluate(X_test, y_test)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    cm = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n",
        "    sns.heatmap(cm, annot=True, fmt='d')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()"
      ]
    }
  ]
}